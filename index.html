<!DOCTYPE html>
<html>

<head>
    <meta charset="UTF-8">
    <title>AI Reading group</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" href="css/header.css">
    <link rel="stylesheet" href="css/timeline.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">


</head>

<body>
    <header>
        <a class="about">About</a>
        <img src="./docs/vaader-air.png" alt="air-logo" width="150">
        <h1 class="text-center">Vaader AI Reading Group</h1>
    </header>
    <ul class="timeline">
        <li class="timeline-item period">
            <div class="timeline-info"></div>
            <div class="timeline-marker"></div>
            <div class="timeline-content">
                <h2 class="timeline-title">2021</h2>
            </div>
        </li>
        <li class="timeline-item">
            <a href="./pages/marie-04-03-21/page.html">
                <div class="timeline-info">
                    <span>mar. 4, 2021</span>
                </div>
                <div class="timeline-marker"></div>
                <div class="timeline-content">
                    <h3 class="timeline-title">ConvNets and humans are not biased towards the same information in images
                    </h3>
                    <h5 class="timeline-title">Alban Marie</h5>
                    <ul class="tags">
                        <li>CNN</li>
                    </ul>
                    <p>
                        Nowadays, it is well established that ConvNets are able to achieve incredible performance on
                        complex
                        vision task such as classification, object recognition or semantic segmentation. A common
                        thought is
                        that humans and ConvNets are able to solve these tasks by learning increasingly complex
                        representations of object shapes. However, recent studies show that humans and ConvNets have
                        indeed
                        very different strategies by not being biased towards the same information in images. To this
                        end,
                        authors propose a stylized version of ImageNet , allowing ConvNets to learn images
                        representation
                        used by humans easier.
                    </p>
                </div>
            </a>
        </li>
        <li class="timeline-item">
            <a href="./pages/beuve-18-02-21/page.html">
                <div class="timeline-info">
                    <span>feb. 18, 2021</span>
                </div>
                <div class="timeline-marker"></div>
                <div class="timeline-content">
                    <h3 class="timeline-title">Image-to-image translation with GANs</h3>
                    <h5 class="timeline-title">Nicolas Beuve</h5>
                    <ul class="tags">
                        <li>GAN</li>
                    </ul>
                    <p>
                        Image-to-image translation is a realm aiming at transposing images from one representation to
                        another, like generating an aerial map of a region based on a photograph. Results in this field
                        were
                        greatly improved since the arrival of GAN models in 2014. GANs (Generative Adversarial Nets) are
                        neural networks, specialized in sample generation. When applied to an image, those models are
                        able
                        to generate convincing samples that are similar to images from a reference dataset while
                        remaining
                        completely original.
                    </p>
                </div>
            </a>
        </li>
        <li class="timeline-item">
            <a href="./pages/peyramaure-04-02-21/page.html">
                <div class="timeline-info">
                    <span>feb. 4, 2021</span>
                </div>
                <div class="timeline-marker"></div>
                <div class="timeline-content">
                    <h3 class="timeline-title">Maybe BERT is all you need ?</h3>
                    <h5 class="timeline-title">Paul Peyramaure</h5>
                    <ul class="tags">
                        <li>Transformer</li>
                    </ul>
                    <p>
                        BERT, which stands for Bidirectional Encoder Representations from Transformer, has been
                        published by
                        a Google AI team in 2018. It has been presented as a new cutting-edge model for Natural Language
                        Processing (NLP). Based on Transformer achitecture, it is design to learn bidirectional
                        representations by considering both the left and right contexts in all its layers.
                        While being initially introduced for NLP tasks, it has recently been used to model other tasks
                        such
                        as action recognition.
                    </p>
                </div>
            </a>
        </li>
        <li class="timeline-item">
            <a href="./pages/lemarchant-21-01-21/page.html">
                <div class="timeline-info">
                    <span>jan. 21, 2021</span>
                </div>
                <div class="timeline-marker"></div>
                <div class="timeline-content">
                    <h3 class="timeline-title">Transformers: Attention is all you need!</h3>
                    <h5 class="timeline-title">FLorian Lemarchant</h5>
                    <ul class="tags">
                        <li>Transformer</li>
                    </ul>
                    <p>
                        While the Transformer architecture has become the de-facto standard for natural language
                        processing tasks, its applications to computer vision remain limited. In vision,
                        attention is either applied in conjunction with convolutional networks, or used to
                        replace certain components of convolutional networks while keeping their overall
                        structure in place. It has been shown that this reliance on CNNs is not necessary and a
                        pure transformer applied directly to sequences of image patches can perform very well on
                        multiple image tasks.
                    </p>
                </div>
            </a>
        </li>
    </ul>


</body>

</html>

<!--
Copyright (c) 2017 by Brady Wright (http://codepen.io/phasethree/pen/NNOvrW)


Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
-->